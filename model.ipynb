{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Devashish\n",
      "[nltk_data]     Uniyal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Devashish\n",
      "[nltk_data]     Uniyal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Devashish\n",
      "[nltk_data]     Uniyal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import joblib\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devashish Uniyal\\AppData\\Local\\Temp\\ipykernel_696\\3358939231.py:1: DtypeWarning: Columns (0,1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sentiment                                               text\n",
      "0  sentiment                                               text\n",
      "1          0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n",
      "2          0  is upset that he can't update his Facebook by ...\n",
      "3          0  @Kenichan I dived many times for the ball. Man...\n",
      "4          0    my whole body feels itchy and like its on fire \n",
      "5          0  @nationwideclass no, it's not behaving at all....\n",
      "6          0                      @Kwesidei not the whole crew \n",
      "7          0                                        Need a hug \n",
      "8          0  @LOLTrish hey  long time no see! Yes.. Rains a...\n",
      "9          0               @Tatiana_K nope they didn't have it \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", encoding=\"latin-1\", header=None)\n",
    "df.columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "print(df[['sentiment', 'text']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 2]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0                                               text   \n",
      "1  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "2  is upset that he can't update his Facebook by ...   \n",
      "3  @Kenichan I dived many times for the ball. Man...   \n",
      "4    my whole body feels itchy and like its on fire    \n",
      "\n",
      "                                          clean_text  \n",
      "0                                               text  \n",
      "1  awww thats bummer shoulda got david carr third...  \n",
      "2  upset cant update facebook texting might cry r...  \n",
      "3       dived many time ball managed save rest bound  \n",
      "4                    whole body feel itchy like fire  \n"
     ]
    }
   ],
   "source": [
    "df[\"clean_text\"] = df[\"text\"].apply(clean_text)\n",
    "df = df[df[\"clean_text\"].str.strip() != \"\"]  # Remove empty entries after cleaning\n",
    "print(df[[\"text\", \"clean_text\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentiment: 0 (negative) => 0, 4 (positive) => 1\n",
    "df = df[df['sentiment'].isin([0, 4])]\n",
    "df['sentiment'] = df['sentiment'].map({0: 0, 4: 1})\n",
    "\n",
    "X = df[\"clean_text\"]\n",
    "y = df[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1168312, Test samples: 292079\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(X_train)}, Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF completed.\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=25000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=5,\n",
    "    max_df=0.9,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "print(\"TF-IDF completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature selection done.\n"
     ]
    }
   ],
   "source": [
    "selector = SelectKBest(chi2, k=15000)  # Reduced features to avoid overfitting\n",
    "X_train_selected = selector.fit_transform(X_train_tfidf, y_train)\n",
    "X_test_selected = selector.transform(X_test_tfidf)\n",
    "print(\"Feature selection done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: {0: 1.0975548349034447, 1: 0.918371646244643}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "weights_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "print(f\"Class Weights: {weights_dict}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Devashish Uniyal\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM training complete.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(\n",
    "    class_weight=weights_dict, \n",
    "    C=0.25\n",
    ")\n",
    "model.fit(X_train_selected, y_train)\n",
    "print(\"SVM training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.7985\n",
      "\n",
      "Train Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.77      0.78    532234\n",
      "           1       0.81      0.82      0.82    636078\n",
      "\n",
      "    accuracy                           0.80   1168312\n",
      "   macro avg       0.80      0.80      0.80   1168312\n",
      "weighted avg       0.80      0.80      0.80   1168312\n",
      "\n",
      "\n",
      "Train Confusion Matrix:\n",
      "[[409956 122278]\n",
      " [113191 522887]]\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train_selected)\n",
    "print(f\"Train Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
    "print(\"\\nTrain Classification Report:\")\n",
    "print(classification_report(y_train, y_train_pred))\n",
    "print(\"\\nTrain Confusion Matrix:\")\n",
    "print(confusion_matrix(y_train, y_train_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.7905\n",
      "\n",
      "Test Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.76      0.77    133059\n",
      "           1       0.80      0.81      0.81    159020\n",
      "\n",
      "    accuracy                           0.79    292079\n",
      "   macro avg       0.79      0.79      0.79    292079\n",
      "weighted avg       0.79      0.79      0.79    292079\n",
      "\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[101344  31715]\n",
      " [ 29466 129554]]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(X_test_selected)\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
    "print(\"\\nTest Classification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and vectorizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Saving our Model\n",
    "joblib.dump(model, \"svc_twitter_sentiment.pkl\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(selector, \"feature_selector.pkl\")\n",
    "print(\"Model and vectorizer saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
